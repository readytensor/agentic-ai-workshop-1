{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic AI Workshop — Part 1: Basic LLM Calls, Streaming, and Chat\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Text-in → Text-out with the OpenAI Python SDK\n",
    "- Text → Image generation\n",
    "- Text → Audio (TTS)\n",
    "- A minimal Gradio UI for an LLM call\n",
    "- Streaming responses\n",
    "- A simple multi‑turn chat\n",
    "\n",
    "> **Prereqs**\n",
    "> - `pip install openai gradio`  \n",
    "> - Set your API key in the environment as `OPENAI_API_KEY` (recommended).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "from IPython.display import Image, display, Audio\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append('../utils')\n",
    "from helpers import load_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and verify API keys\n",
    "load_env(api_key_type=\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs directory\n",
    "OUTPUTS_DIR = './../outputs/'\n",
    "os.makedirs(OUTPUTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Initialize OpenAI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "# This will automatically use the OPENAI_API_KEY environment variable that was loaded\n",
    "# You can also explicitly pass the key with: OpenAI(api_key=\"sk-...\")\n",
    "client = OpenAI()  # Uses OPENAI_API_KEY from environment\n",
    "print(\"Client ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test basic text completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_turn_completion(prompt: str) -> str:\n",
    "    \"\"\"Get a single-turn completion from the LLM (no conversation history)\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "prompt = \"Write a friendly, two-sentence definition of Retrieval-Augmented Generation.\"\n",
    "\n",
    "print(\"\\nUser Prompt:\")\n",
    "print(\"-\" * 40)\n",
    "print(prompt)\n",
    "print(\"-\" * 40)\n",
    "\n",
    "result = get_single_turn_completion(prompt)\n",
    "\n",
    "print(\"\\n LLM Response:\")\n",
    "print(\"-\" * 40)\n",
    "print(result) \n",
    "print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Streaming Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_response(prompt):\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    response_text = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            chunk_text = chunk.choices[0].delta.content\n",
    "            response_text += chunk_text\n",
    "            print(chunk_text, end=\"\", flush=True)\n",
    "    return response_text\n",
    "\n",
    "# Test streaming\n",
    "prompt = \"Write a short 300 words story about a robot who discovers emotions.\"\n",
    "print(\"\\nUser Prompt:\")\n",
    "print(\"-\" * 40)\n",
    "print(prompt)\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nStreaming response:\")\n",
    "print(\"-\" * 40)\n",
    "response = stream_response(prompt)\n",
    "print(\"\\n\" + \"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Image Generation - Text to Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(prompt):\n",
    "    \"\"\"Generate an image from text\"\"\"\n",
    "    response = client.images.generate(\n",
    "        model=\"dall-e-3\",\n",
    "        prompt=prompt,\n",
    "        size=\"1024x1024\",\n",
    "        quality=\"standard\", \n",
    "        n=1,\n",
    "    )\n",
    "    return response.data[0].url\n",
    "\n",
    "# Test image generation\n",
    "image_url = generate_image(\n",
    "    \"Flat image of a helpful robot librarian fetching a book titled 'RAG' \"\n",
    "    \"from a bookshelf. Teal shade.\")\n",
    "\n",
    "# Download the image\n",
    "image_response = requests.get(image_url, timeout=20)\n",
    "\n",
    "# Print image size\n",
    "image_size = len(image_response.content)\n",
    "print(f\"Image size: {image_size:,} bytes\")\n",
    "\n",
    "# Save the image\n",
    "output_path = os.path.join(OUTPUTS_DIR, 'generated_image.png')\n",
    "with open(output_path, 'wb') as f:\n",
    "    f.write(image_response.content)\n",
    "print(f\"Image saved to: {output_path}\")\n",
    "\n",
    "# Display the image\n",
    "display(Image(url=image_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Audio Generation - Text to Speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_speech(text, voice=\"fable\"):\n",
    "    \"\"\"\n",
    "    Generate speech from text\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to generate speech from\n",
    "        voice (str): The voice to use for the speech (default is \"alloy\")\n",
    "                     Options: \"alloy\", \"echo\", \"fable\", \"onyx\", \"nova\", \"shimmer\"\n",
    "\n",
    "    Returns:\n",
    "        bytes: The generated audio content\n",
    "    \"\"\"\n",
    "    response = client.audio.speech.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=voice,\n",
    "        input=text\n",
    "    )\n",
    "    return response.content\n",
    "\n",
    "# Test speech generation\n",
    "audio_content = generate_speech(\n",
    "    \"Hello! Welcome to the Agentic AI Workshop. \"\n",
    "    \"Let's build some amazing AI applications together!\")\n",
    "\n",
    "# Print audio size\n",
    "print(f\"Generated audio content: {len(audio_content)} bytes\")\n",
    "\n",
    "# Save the audio\n",
    "audio_fpath = Path(OUTPUTS_DIR) / 'generated_audio.mp3'\n",
    "with open(audio_fpath, 'wb') as f:\n",
    "    f.write(audio_content)\n",
    "print(f\"Audio saved to: {audio_fpath}\")\n",
    "\n",
    "# Play inline\n",
    "Audio(audio_content, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gradio UI for Interactive Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Message Format:\n",
    "- List of message objects with \"role\" and \"content\" fields\n",
    "- Roles: \"system\" (instructions), \"user\" (human input), \"assistant\" (AI response)\n",
    "- Example: [{\"role\": \"user\", \"content\": \"Hello\"}, {\"role\": \"assistant\", \"content\": \"Hi!\"}]\n",
    "\n",
    "Roles can be system, user, assistant, tool\n",
    "\n",
    "### Gradio Message Format:\n",
    "- For basic Interface: Single input string passed directly to function\n",
    "- For ChatInterface: Tuple of (current_message, list_of_message_pairs)\n",
    "- Message pairs are (user_message, assistant_message) tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple Gradio interface for text generation\n",
    "def chat_with_llm(message):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Create Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=chat_with_llm,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"Basic LLM Chat\",\n",
    "    description=\"A simple chat interface with GPT-3.5-turbo\",\n",
    "    examples=[\n",
    "        \"Tell me a joke\",\n",
    "        \"Explain quantum computing in simple terms\",\n",
    "        \"Write a haiku about AI\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multi-turn Chat Conversation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Multi-turn chat with conversation history\n",
    "def chat_with_history(message, history):\n",
    "    \"\"\"Chat function with conversation history.\"\"\"\n",
    "    # Start with system message if this is the first turn\n",
    "    if not history:\n",
    "        messages = [{\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}]\n",
    "    else:\n",
    "        messages = []\n",
    "    \n",
    "    # Add conversation history\n",
    "    messages.extend(history)\n",
    "    \n",
    "    # Add current user message\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Create advanced chat interface\n",
    "chat_interface = gr.ChatInterface(\n",
    "    fn=chat_with_history,\n",
    "    title=\"Advanced AI Chat\", \n",
    "    description=\"A multi-turn chat interface with conversation memory - maintains full chat history\",\n",
    "    examples=[\n",
    "        \"Hello! What can you help me with?\",\n",
    "        \"Tell me about machine learning\", \n",
    "        \"Can you write a Python function?\"\n",
    "    ],\n",
    "    cache_examples=False,\n",
    "    type=\"messages\"  # Use the new messages format instead of deprecated tuples\n",
    ")\n",
    "\n",
    "# Launch the advanced interface\n",
    "chat_interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 1: Add Streaming to Multi-Turn Chatbot\n",
    "\n",
    "Your task: Modify the chat function to stream responses token-by-token instead of \n",
    "waiting for the complete response.\n",
    "\n",
    "## EXERCISE 2: Add Text-to-Speech to Your Chatbot\n",
    "\n",
    "Your task: Create a chatbot that streams the text response AND generates audio \n",
    "of the AI's response. The AI Assistant should speak its response while also streaming it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
