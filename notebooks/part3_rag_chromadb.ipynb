{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f09c466b",
   "metadata": {},
   "source": [
    "# Agentic AI Workshop — Part 3: RAG Chatbot with ChromaDB\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Ingesting a small set of documents\n",
    "- Chunking + embedding with OpenAI Embeddings\n",
    "- Storing & retrieving with ChromaDB\n",
    "- Building a simple RAG prompt\n",
    "- Gradio Chat UI that does retrieval on each turn\n",
    "\n",
    "> **Prereqs**\n",
    "> - `pip install chromadb openai gradio tiktoken` (tiktoken optional for token-aware chunking)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f866bc2c",
   "metadata": {},
   "source": [
    "# If needed, uncomment to install:\n",
    "# !pip install --upgrade chromadb openai gradio tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d20e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, textwrap, uuid\n",
    "from typing import List, Dict\n",
    "\n",
    "# Load environment variables from .env file\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from helpers import load_environment, get_env_var\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "\n",
    "# Load environment variables\n",
    "load_environment()\n",
    "print(\"Environment variables loaded successfully!\")\n",
    "\n",
    "# Verify API key is loaded\n",
    "api_key = get_env_var('OPENAI_API_KEY')\n",
    "if not api_key or api_key == 'your_openai_api_key_here':\n",
    "    print(\"⚠️  Please set your OPENAI_API_KEY in the .env file\")\n",
    "else:\n",
    "    print(\"✅ OpenAI API key found!\")\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Configure Chroma (in-memory for workshop; switch to persistent_dir for disk)\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"workshop_docs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb16791e",
   "metadata": {},
   "source": [
    "# --- Source documents\n",
    "# Replace these with real Ready Tensor publication texts or load from files.\n",
    "source_docs = [\n",
    "    {\n",
    "        \"id\": \"rt_pub_1\",\n",
    "        \"title\": \"Benchmarking Forecasting Models\",\n",
    "        \"text\": \"We benchmark classical, ML, and LLM-based forecasting models across many datasets using metrics such as RMSE and sMAPE.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"rt_pub_2\",\n",
    "        \"title\": \"TSP Dataset Generator\",\n",
    "        \"text\": \"An open-source tool to generate large-scale TSP datasets for experimentation with combinatorial optimization and LLMs.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"rt_pub_3\",\n",
    "        \"title\": \"Distance Profile for Time Series\",\n",
    "        \"text\": \"Distance profiling enables tasks like classification, motif discovery, and anomaly detection in time series analysis.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"rt_pub_4\",\n",
    "        \"title\": \"Handling Class Imbalance in Binary Classification\",\n",
    "        \"text\": \"We compare SMOTE, class weights, and decision-threshold adjustment across models and datasets with extensive metrics.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"rt_pub_5\",\n",
    "        \"title\": \"Evaluating Multi-Agent AI Systems\",\n",
    "        \"text\": \"We evaluate a multi-agent authoring assistant using RAGAS and custom metrics focusing on coherence and utility.\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0da1aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Chunking utility (simple)\n",
    "def chunk_text(text: str, max_chars: int = 800, overlap: int = 100) -> List[str]:\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(text)\n",
    "    while start < n:\n",
    "        end = min(n, start + max_chars)\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end - overlap\n",
    "        if start < 0:\n",
    "            start = 0\n",
    "        if start >= n:\n",
    "            break\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef1cc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Embedding & upsert\n",
    "def embed_texts(texts: List[str], model: str = \"text-embedding-3-small\") -> List[List[float]]:\n",
    "    resp = client.embeddings.create(model=model, input=texts)\n",
    "    return [d.embedding for d in resp.data]\n",
    "\n",
    "def ingest_documents(docs: List[Dict]):\n",
    "    ids, docs_texts, metadatas = [], [], []\n",
    "    for d in docs:\n",
    "        chunks = chunk_text(d[\"text\"])\n",
    "        for i, ch in enumerate(chunks):\n",
    "            ids.append(f\"{d['id']}_{i}\")\n",
    "            docs_texts.append(ch)\n",
    "            metadatas.append({\"title\": d[\"title\"], \"parent_id\": d[\"id\"], \"chunk\": i})\n",
    "    embs = embed_texts(docs_texts)\n",
    "    collection.upsert(ids=ids, documents=docs_texts, metadatas=metadatas, embeddings=embs)\n",
    "    return len(ids)\n",
    "\n",
    "num_chunks = ingest_documents(source_docs)\n",
    "print(f\"Ingested {num_chunks} chunks into Chroma.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fe0036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Retrieval\n",
    "def retrieve(query: str, k: int = 3):\n",
    "    q_emb = embed_texts([query])[0]\n",
    "    res = collection.query(query_embeddings=[q_emb], n_results=k, include=[\"documents\",\"metadatas\",\"distances\"])\n",
    "    hits = []\n",
    "    for doc, meta, dist in zip(res[\"documents\"][0], res[\"metadatas\"][0], res[\"distances\"][0]):\n",
    "        hits.append({\"text\": doc, \"meta\": meta, \"distance\": float(dist)})\n",
    "    return hits\n",
    "\n",
    "# Test retrieval\n",
    "for h in retrieve(\"How do you compare models on forecasting tasks?\", k=2):\n",
    "    print(h[\"meta\"][\"title\"], \"→\", h[\"distance\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197039dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- RAG prompt\n",
    "SYSTEM = \"You are a helpful RAG assistant. Answer based only on the provided CONTEXT. If the answer is not in context, say you don't know.\"\n",
    "USER_TEMPLATE = \"QUESTION: {question}\\n\\nCONTEXT:\\n{context}\"\n",
    "\n",
    "def build_context(hits):\n",
    "    ctx = []\n",
    "    for h in hits:\n",
    "        title = h[\"meta\"][\"title\"]\n",
    "        chunk_id = h[\"meta\"][\"chunk\"]\n",
    "        ctx.append(f\"[{title} / chunk {chunk_id}]\\n{h['text']}\")\n",
    "    return \"\\n\\n\".join(ctx[:3])\n",
    "\n",
    "def rag_answer(question: str):\n",
    "    hits = retrieve(question, k=4)\n",
    "    context = build_context(hits)\n",
    "    prompt = USER_TEMPLATE.format(question=question, context=context)\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    return resp.choices[0].message.content, hits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31479dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Gradio Chat UI with retrieval per turn\n",
    "with gr.Blocks(title=\"RAG Chatbot (ChromaDB)\") as app:\n",
    "    gr.Markdown(\"### RAG Chatbot — ask about the sample Ready Tensor publications\")\n",
    "    chat = gr.Chatbot(height=300)\n",
    "    q = gr.Textbox(placeholder=\"Ask a question about the publications...\", label=\"Your question\")\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def chat_step(history, user_msg):\n",
    "        answer, hits = rag_answer(user_msg)\n",
    "        cited = \"\\n\\n**Sources:**\\n\" + \"\\n\".join(\n",
    "            f\"- {h['meta']['title']} (chunk {h['meta']['chunk']})\" for h in hits[:3]\n",
    "        )\n",
    "        history = history + [(user_msg, answer + cited)]\n",
    "        return history, \"\"\n",
    "\n",
    "    def do_clear():\n",
    "        return []\n",
    "\n",
    "    q.submit(chat_step, [chat, q], [chat, q])\n",
    "    clear.click(do_clear, [], [chat])\n",
    "\n",
    "# Uncomment to run locally\n",
    "# app.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a236a163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Chunking utility (simple)\n",
    "def chunk_text(text: str, max_chars: int = 800, overlap: int = 100) -> List[str]:\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(text)\n",
    "    while start < n:\n",
    "        end = min(n, start + max_chars)\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end - overlap\n",
    "        if start < 0:\n",
    "            start = 0\n",
    "        if start >= n:\n",
    "            break\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57643fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Embedding & upsert\n",
    "def embed_texts(texts: List[str], model: str = \"text-embedding-3-small\") -> List[List[float]]:\n",
    "    resp = client.embeddings.create(model=model, input=texts)\n",
    "    return [d.embedding for d in resp.data]\n",
    "\n",
    "def ingest_documents(docs: List[Dict]):\n",
    "    ids, docs_texts, metadatas = [], [], []\n",
    "    for d in docs:\n",
    "        chunks = chunk_text(d[\"text\"])\n",
    "        for i, ch in enumerate(chunks):\n",
    "            ids.append(f\"{d['id']}_{i}\")\n",
    "            docs_texts.append(ch)\n",
    "            metadatas.append({\"title\": d[\"title\"], \"parent_id\": d[\"id\"], \"chunk\": i})\n",
    "    embs = embed_texts(docs_texts)\n",
    "    collection.upsert(ids=ids, documents=docs_texts, metadatas=metadatas, embeddings=embs)\n",
    "    return len(ids)\n",
    "\n",
    "num_chunks = ingest_documents(source_docs)\n",
    "print(f\"Ingested {num_chunks} chunks into Chroma.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbb932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Retrieval\n",
    "def retrieve(query: str, k: int = 3):\n",
    "    q_emb = embed_texts([query])[0]\n",
    "    res = collection.query(query_embeddings=[q_emb], n_results=k, include=[\"documents\",\"metadatas\",\"distances\"])\n",
    "    hits = []\n",
    "    for doc, meta, dist in zip(res[\"documents\"][0], res[\"metadatas\"][0], res[\"distances\"][0]):\n",
    "        hits.append({\"text\": doc, \"meta\": meta, \"distance\": float(dist)})\n",
    "    return hits\n",
    "\n",
    "# Test retrieval\n",
    "for h in retrieve(\"How do you compare models on forecasting tasks?\", k=2):\n",
    "    print(h[\"meta\"][\"title\"], \"→\", h[\"distance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0110e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- RAG prompt\n",
    "SYSTEM = \"You are a helpful RAG assistant. Answer based only on the provided CONTEXT. If the answer is not in context, say you don't know.\"\n",
    "USER_TEMPLATE = \"QUESTION: {question}\\n\\nCONTEXT:\\n{context}\"\n",
    "\n",
    "def build_context(hits):\n",
    "    ctx = []\n",
    "    for h in hits:\n",
    "        title = h[\"meta\"][\"title\"]\n",
    "        chunk_id = h[\"meta\"][\"chunk\"]\n",
    "        ctx.append(f\"[{title} / chunk {chunk_id}]\\n{h['text']}\")\n",
    "    return \"\\n\\n\".join(ctx[:3])\n",
    "\n",
    "def rag_answer(question: str):\n",
    "    hits = retrieve(question, k=4)\n",
    "    context = build_context(hits)\n",
    "    prompt = USER_TEMPLATE.format(question=question, context=context)\n",
    "    resp = client.responses.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    return resp.output_text, hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efb2bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Gradio Chat UI with retrieval per turn\n",
    "with gr.Blocks(title=\"RAG Chatbot (ChromaDB)\") as app:\n",
    "    gr.Markdown(\"### RAG Chatbot — ask about the sample Ready Tensor publications\")\n",
    "    chat = gr.Chatbot(height=300)\n",
    "    q = gr.Textbox(placeholder=\"Ask a question about the publications...\", label=\"Your question\")\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def chat_step(history, user_msg):\n",
    "        answer, hits = rag_answer(user_msg)\n",
    "        cited = \"\\n\\n**Sources:**\\n\" + \"\\n\".join(\n",
    "            f\"- {h['meta']['title']} (chunk {h['meta']['chunk']})\" for h in hits[:3]\n",
    "        )\n",
    "        history = history + [(user_msg, answer + cited)]\n",
    "        return history, \"\"\n",
    "\n",
    "    def do_clear():\n",
    "        return []\n",
    "\n",
    "    q.submit(chat_step, [chat, q], [chat, q])\n",
    "    clear.click(do_clear, [], [chat])\n",
    "\n",
    "# Uncomment to run locally\n",
    "# app.launch()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
